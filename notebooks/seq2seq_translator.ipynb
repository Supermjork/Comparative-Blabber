{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItoot6aut2h"
      },
      "source": [
        "# Seq2Seq Neural Machine Translation\n",
        "\n",
        "This notebook implements a sequence-to-sequence model for machine translation. The implementation is organized into modular sections for clarity and maintainability.\n",
        "\n",
        "## Table of Contents\n",
        "1. Imports and Setup\n",
        "2. Data Handling\n",
        "3. Model Architecture\n",
        "4. Training Utilities\n",
        "5. Translation Utilities\n",
        "6. Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnHluGBut2i"
      },
      "source": [
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4dMrM9E7ut2i",
        "outputId": "12ba2085-8bc5-4fdb-9b59-7003117b10fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN8mO3Xtut2j"
      },
      "source": [
        "## 2. Data Handling\n",
        "\n",
        "### 2.1 Vocabulary Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h80o25vEut2j"
      },
      "outputs": [],
      "source": [
        "def build_vocab(token_list: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], int]:\n",
        "    \"\"\"Build vocabulary from list of tokens.\"\"\"\n",
        "    # Count token frequencies\n",
        "    counter = Counter(token_list)\n",
        "\n",
        "    # Filter by minimum frequency\n",
        "    filtered_tokens = [token for token, count in counter.items() if count >= min_freq]\n",
        "\n",
        "    # Create vocabulary with special tokens\n",
        "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "    token_to_idx = {token: idx for idx, token in enumerate(special_tokens + filtered_tokens)}\n",
        "\n",
        "    return token_to_idx, len(token_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1h9Zx5Tut2k"
      },
      "source": [
        "### 2.2 Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PvwGHVpzut2k"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, src_vocab, tgt_vocab, src_col=\"en\", tgt_col=\"it\"):\n",
        "        self.src_sentences = dataframe[src_col].to_list()\n",
        "        self.tgt_sentences = dataframe[tgt_col].to_list()\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert tokens to indices using vocabulary\n",
        "        src_indices = [self.src_vocab.get(token, 0) for token in self.src_sentences[idx]]\n",
        "        tgt_indices = [self.tgt_vocab.get(token, 0) for token in self.tgt_sentences[idx]]\n",
        "\n",
        "        return {\n",
        "            \"src\": torch.tensor(src_indices, dtype=torch.long),\n",
        "            \"tgt\": torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgdTrHZcut2k"
      },
      "source": [
        "### 2.3 Data Loading Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UP-IT_yIut2k"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for padding sequences in a batch.\"\"\"\n",
        "    src_batch = [item['src'] for item in batch]\n",
        "    tgt_batch = [item['tgt'] for item in batch]\n",
        "\n",
        "    # Pad sequences in the batch to the same length\n",
        "    src_padded = rnn_utils.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_padded = rnn_utils.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\"src\": src_padded, \"tgt\": tgt_padded}\n",
        "\n",
        "def create_dataloaders(train_dataset, val_dataset, batch_size, shuffle=True):\n",
        "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BwVQ7h2ut2l"
      },
      "source": [
        "## 3. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cBvBG1izut2l"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, emb_dim: int,\n",
        "                 hid_dim: int, n_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Embedding(input_dim, emb_dim)\n",
        "        self.decoder = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Encode source\n",
        "        src_emb = self.dropout(self.encoder(src))\n",
        "        _, (hidden, cell) = self.rnn(src_emb)\n",
        "\n",
        "        # Decode target\n",
        "        tgt_emb = self.dropout(self.decoder(tgt))\n",
        "        outputs, _ = self.rnn(tgt_emb, (hidden, cell))\n",
        "\n",
        "        predictions = self.fc_out(outputs)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5vaTro_ut2m"
      },
      "source": [
        "## 4. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d96LUZhiut2m"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    epochs: int,\n",
        "    learning_rate: float,\n",
        "    device: torch.device\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "    \"\"\"Train the model and return training history.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt[:, :-1])\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def validate_model(\n",
        "    model: nn.Module,\n",
        "    val_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"Validate the model and return validation loss.\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "def plot_training_history(train_losses: List[float], val_losses: List[float]):\n",
        "    \"\"\"Plot training and validation loss history.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNXIk-rut2m"
      },
      "source": [
        "## 5. Translation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wJiLTkwfut2m"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(\n",
        "    model: nn.Module,\n",
        "    sentence: list,\n",
        "    src_vocab: dict,\n",
        "    tgt_vocab: dict,\n",
        "    device: torch.device,\n",
        "    max_len: int = 50\n",
        ") -> list:\n",
        "    \"\"\"Translate a sentence using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    src_indices = [src_vocab.get(token, src_vocab['<unk>']) for token in sentence]\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.encoder(src_tensor)\n",
        "        _, (hidden, cell) = model.rnn(src_emb)\n",
        "\n",
        "    # Initialize with start token\n",
        "    outputs = [tgt_vocab['<sos>']]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor(outputs, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(tgt_tensor)\n",
        "            output, (hidden, cell) = model.rnn(output, (hidden, cell))\n",
        "            pred_token = output.argmax(2)[:, -1].item()\n",
        "\n",
        "        outputs.append(pred_token)\n",
        "        if pred_token == tgt_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Convert indices back to tokens\n",
        "    idx_to_token = {idx: token for token, idx in tgt_vocab.items()}\n",
        "    translated_tokens = [idx_to_token.get(idx, '<unk>') for idx in outputs[1:-1]]  # Remove <sos> and <eos>\n",
        "\n",
        "    return translated_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JicXMxmut2m"
      },
      "source": [
        "## 6. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xzRdTe4Jut2n",
        "outputId": "80135f3e-f3ec-4fa1-f42b-7fb64a7c1cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 2)\n",
              "┌─────────────────────────────────┬─────────────────────────────────┐\n",
              "│ it                              ┆ en                              │\n",
              "│ ---                             ┆ ---                             │\n",
              "│ list[str]                       ┆ list[str]                       │\n",
              "╞═════════════════════════════════╪═════════════════════════════════╡\n",
              "│ [\"grazie\", \"amico\"]             ┆ [\"thank\", \"buddy\"]              │\n",
              "│ [\"di il\"]                       ┆ [\"say\"]                         │\n",
              "│ [\"trifosfare\", \"sodio\", … \"sod… ┆ [\"sodium\", \"triphosphate\", … \"… │\n",
              "│ [\"invero\", \"avidare\", … \"ricch… ┆ [\"surely\", \"ardent\", … \"wealth… │\n",
              "│ [\"allegare\"]                    ┆ [\"annex\"]                       │\n",
              "└─────────────────────────────────┴─────────────────────────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>it</th><th>en</th></tr><tr><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>[&quot;grazie&quot;, &quot;amico&quot;]</td><td>[&quot;thank&quot;, &quot;buddy&quot;]</td></tr><tr><td>[&quot;di il&quot;]</td><td>[&quot;say&quot;]</td></tr><tr><td>[&quot;trifosfare&quot;, &quot;sodio&quot;, … &quot;sodio&quot;]</td><td>[&quot;sodium&quot;, &quot;triphosphate&quot;, … &quot;tripolyphosphate&quot;]</td></tr><tr><td>[&quot;invero&quot;, &quot;avidare&quot;, … &quot;ricchezzo&quot;]</td><td>[&quot;surely&quot;, &quot;ardent&quot;, … &quot;wealth&quot;]</td></tr><tr><td>[&quot;allegare&quot;]</td><td>[&quot;annex&quot;]</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Configuration\n",
        "BATCH_SIZE = 64\n",
        "EMB_DIM = 128\n",
        "HID_DIM = 258\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 5\n",
        "\n",
        "# Load data\n",
        "local_path = \"../data/output/processed.parquet\"\n",
        "colab_path = \"../content/processed.parquet\"\n",
        "input_ready = pl.read_parquet(colab_path)\n",
        "print(\"Data loaded successfully\")\n",
        "input_ready.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MfMMHdwmut2n",
        "outputId": "8bcfdb3a-7501-4976-840a-5ed0f4469035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 130049\n",
            "Italian Vocabulary Size: 198442\n"
          ]
        }
      ],
      "source": [
        "# Build vocabularies\n",
        "english_tokens = [token for sentence in input_ready[\"en\"].to_list() for token in sentence]\n",
        "italian_tokens = [token for sentence in input_ready[\"it\"].to_list() for token in sentence]\n",
        "\n",
        "english_vocab, input_dim = build_vocab(english_tokens)\n",
        "italian_vocab, output_dim = build_vocab(italian_tokens)\n",
        "\n",
        "print(f\"English Vocabulary Size: {input_dim}\")\n",
        "print(f\"Italian Vocabulary Size: {output_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vIkkjOHwut2n",
        "outputId": "02c7d217-b3b0-4716-e840-408e81f0fbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets and dataloaders created successfully\n"
          ]
        }
      ],
      "source": [
        "# Split data and create datasets\n",
        "train_data, val_data = train_test_split(input_ready, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TranslationDataset(train_data, english_vocab, italian_vocab)\n",
        "val_dataset = TranslationDataset(val_data, english_vocab, italian_vocab)\n",
        "\n",
        "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, BATCH_SIZE)\n",
        "print(\"Datasets and dataloaders created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "86Ajp88cut2n",
        "outputId": "5d7d7943-5cea-4c80-cc16-0dec1e381814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.57 GiB is free. Process 2550 has 12.17 GiB memory in use. Of the allocated memory 11.18 GiB is allocated by PyTorch, and 876.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-11fef4c40150>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-68cb3fec22de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.57 GiB is free. Process 2550 has 12.17 GiB memory in use. Of the allocated memory 11.18 GiB is allocated by PyTorch, and 876.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Initialize and train model\n",
        "model = Seq2Seq(input_dim, output_dim, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, device)\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psPACdd6ut2o"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"models/translator.pt\")\n",
        "print(\"Model saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUWZL5paut2o"
      },
      "outputs": [],
      "source": [
        "# Test translation\n",
        "test_sentence = [\"hello\", \"world\"]\n",
        "translated = translate_sentence(model, test_sentence, english_vocab, italian_vocab, device)\n",
        "print(f\"Input: {' '.join(test_sentence)}\")\n",
        "print(f\"Translation: {' '.join(translated)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}