{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "After having done $2$ assignments, I have deemed it fit to factor out preprocessing into its own notebook, instead of having it attached to the first model so as to not hinder its execution or add overhead.\n",
    "\n",
    "The data required to be used by the models shall be stored under `<arbitrary path>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Loading the `.parquet` files for training. I am unsure which language to try first.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import polars as pl\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which language\n",
    "# we want to train for\n",
    "lang = \"italian\"\n",
    "\n",
    "lang_abbr = {\"italian\": \"it\",\n",
    "             \"french\": \"fr\",}\n",
    "\n",
    "# Prepare the path\n",
    "training_data_path = os.path.join(\"..\", \"data\", \"input_lang\", lang, \"training.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;- Grazie, amico. ###&gt;- Thanks,…</td></tr><tr><td>&quot;Dillo. ###&gt;Say it.&quot;</td></tr><tr><td>&quot;Trifosfato di sodio (tripolifo…</td></tr><tr><td>&quot;Invero è avido per amore delle…</td></tr><tr><td>&quot;ALLEGATO I ###&gt;ANNEX I&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 1)\n",
       "┌─────────────────────────────────┐\n",
       "│ text                            │\n",
       "│ ---                             │\n",
       "│ str                             │\n",
       "╞═════════════════════════════════╡\n",
       "│ - Grazie, amico. ###>- Thanks,… │\n",
       "│ Dillo. ###>Say it.              │\n",
       "│ Trifosfato di sodio (tripolifo… │\n",
       "│ Invero è avido per amore delle… │\n",
       "│ ALLEGATO I ###>ANNEX I          │\n",
       "└─────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_parquet_data = pl.read_parquet(training_data_path)\n",
    "\n",
    "raw_parquet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>it</th><th>english</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;- Grazie, amico.&quot;</td><td>&quot;- Thanks, buddy.&quot;</td></tr><tr><td>&quot;Dillo.&quot;</td><td>&quot;Say it.&quot;</td></tr><tr><td>&quot;Trifosfato di sodio (tripolifo…</td><td>&quot;Sodium triphosphate (sodium tr…</td></tr><tr><td>&quot;Invero è avido per amore delle…</td><td>&quot;Surely, he is ardent in his lo…</td></tr><tr><td>&quot;ALLEGATO I&quot;</td><td>&quot;ANNEX I&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ it                              ┆ english                         │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ - Grazie, amico.                ┆ - Thanks, buddy.                │\n",
       "│ Dillo.                          ┆ Say it.                         │\n",
       "│ Trifosfato di sodio (tripolifo… ┆ Sodium triphosphate (sodium tr… │\n",
       "│ Invero è avido per amore delle… ┆ Surely, he is ardent in his lo… │\n",
       "│ ALLEGATO I                      ┆ ANNEX I                         │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = raw_parquet_data.select(\n",
    "    raw_parquet_data[\"text\"]\n",
    "    .str.split(\" ###>\")\n",
    "    .list.to_struct(n_field_strategy = \"max_width\",\n",
    "                    fields = [lang_abbr[lang], \"english\"])\n",
    "    .alias(\"splitten\")\n",
    ").unnest(\"splitten\")\n",
    "\n",
    "split_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Casing and Regex\n",
    "A cruical part of regex is to lowercase the letters so that we are working with the least possible amount of characters in a language's given alphabet.\n",
    "\n",
    "As well as removing most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining path to install NLTK libraries in\n",
    "NLTK_LIB_PATH = os.path.join(\"..\", \"venv_nlp\", \"Lib\", \"nltk_data\")\n",
    "\n",
    "# Defining download function\n",
    "def download_libs():\n",
    "    libraries = {\n",
    "        os.path.join(\"corpora\", \"stopwords\"): \"stopwords\",\n",
    "        os.path.join(\"corpora\",\"wordnet\"): \"wordnet\",\n",
    "        os.path.join(\"tokenizers\", \"punkt\"): \"punkt\"\n",
    "    }\n",
    "\n",
    "    for _, package in libraries.items():\n",
    "        try:\n",
    "            nltk.data.find(package)\n",
    "            print(f\"{package.capitalize()} data exists.\")\n",
    "        except LookupError:\n",
    "            print(f\"Downloading {package}...\")\n",
    "\n",
    "            nltk.download(package, download_dir=NLTK_LIB_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error checking {package}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NLTK data directory: ..\\venv_nlp\\Lib\\nltk_data\n",
      "Downloading stopwords...\n",
      "Downloading wordnet...\n",
      "Downloading punkt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     ..\\venv_nlp\\Lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     ..\\venv_nlp\\Lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to ..\\venv_nlp\\Lib\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(NLTK_LIB_PATH, exist_ok=True)\n",
    "    print(f\"Using NLTK data directory: {NLTK_LIB_PATH}\")\n",
    "    download_libs()\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create or write to directory '{NLTK_LIB_PATH}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_text(text: str) -> str:\n",
    "    '''\n",
    "    Cleans any passed string.\n",
    "    Includes accented characters\n",
    "    '''\n",
    "    text = text.lower()\n",
    "\n",
    "    regex_patterns = [\n",
    "        (r\"https?://\\S+\", \"\"),    # Remove http or https links\n",
    "        (r\"\\S+@\\S+\\.\\S+\", \" \"),   # Remove email addresses\n",
    "        (r\"[^\\w\\sÀ-ÿ'’]\", \" \"),   # Remove special characters, except accented ones\n",
    "        (r\"\\s+\", \" \"),            # Replace multiple spaces with a single space\n",
    "        (r\"^\\s+|\\s+$\", \"\")        # Strip leading and trailing spaces\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in regex_patterns:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords.update(nltk.corpus.stopwords.words(f\"{lang}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(text: str, stopwords_set: set) -> str:\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in stopwords_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(f\"{lang_abbr[lang]}_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading language model\n",
    "lang_model_en = spacy.load(\"en_core_web_sm\")\n",
    "lang_model_other = spacy.load(f\"{lang_abbr[lang]}_core_news_sm\")\n",
    "\n",
    "# Lemmatiser\n",
    "def lemma(tokens, language_model):\n",
    "    doc = language_model(tokens)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
