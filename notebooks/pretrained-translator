{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:13:53.651419Z","iopub.execute_input":"2024-12-21T08:13:53.651845Z","iopub.status.idle":"2024-12-21T08:13:58.264577Z","shell.execute_reply.started":"2024-12-21T08:13:53.651800Z","shell.execute_reply":"2024-12-21T08:13:58.262945Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:13:58.266042Z","iopub.execute_input":"2024-12-21T08:13:58.266517Z","iopub.status.idle":"2024-12-21T08:14:01.788639Z","shell.execute_reply.started":"2024-12-21T08:13:58.266480Z","shell.execute_reply":"2024-12-21T08:14:01.787089Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_name = 'Helsinki-NLP/opus-mt-en-it'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:14:01.789985Z","iopub.execute_input":"2024-12-21T08:14:01.790628Z","iopub.status.idle":"2024-12-21T08:14:01.795894Z","shell.execute_reply.started":"2024-12-21T08:14:01.790580Z","shell.execute_reply":"2024-12-21T08:14:01.794482Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"tokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:14:01.797237Z","iopub.execute_input":"2024-12-21T08:14:01.797581Z","iopub.status.idle":"2024-12-21T08:14:04.630138Z","shell.execute_reply.started":"2024-12-21T08:14:01.797549Z","shell.execute_reply":"2024-12-21T08:14:04.628855Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def translate(text):\n    # Tokenize the input text\n    tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n    \n    # Perform the translation\n    translation = model.generate(**tokenized_text)\n    \n    # Decode the translated text\n    translated_text = tokenizer.decode(translation[0], skip_special_tokens=True)\n    \n    return translated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:14:04.632786Z","iopub.execute_input":"2024-12-21T08:14:04.633164Z","iopub.status.idle":"2024-12-21T08:14:04.639238Z","shell.execute_reply.started":"2024-12-21T08:14:04.633130Z","shell.execute_reply":"2024-12-21T08:14:04.637560Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(translate(\"Hello, how are you?\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T08:14:04.640641Z","iopub.execute_input":"2024-12-21T08:14:04.641075Z","iopub.status.idle":"2024-12-21T08:14:05.127413Z","shell.execute_reply.started":"2024-12-21T08:14:04.641034Z","shell.execute_reply":"2024-12-21T08:14:05.126191Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4252: FutureWarning: \n`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n`__call__` method to prepare your inputs and targets.\n\nHere is a short example:\n\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n\nIf you either need to use different keyword arguments for the source and target texts, you should do two calls like\nthis:\n\nmodel_inputs = tokenizer(src_texts, ...)\nlabels = tokenizer(text_target=tgt_texts, ...)\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\n\nSee the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\nFor a more complete example, see the implementation of `prepare_seq2seq_batch`.\n\n  warnings.warn(formatted_warning, FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"Ciao, come stai?\n","output_type":"stream"}],"execution_count":6}]}