{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10225541,"sourceType":"datasetVersion","datasetId":6321785},{"sourceId":214121457,"sourceType":"kernelVersion"},{"sourceId":214130279,"sourceType":"kernelVersion"},{"sourceId":214130599,"sourceType":"kernelVersion"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport polars as pl\nfrom collections import defaultdict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:22.474389Z","iopub.execute_input":"2024-12-22T08:39:22.474712Z","iopub.status.idle":"2024-12-22T08:39:25.479715Z","shell.execute_reply.started":"2024-12-22T08:39:22.474686Z","shell.execute_reply":"2024-12-22T08:39:25.478795Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:25.480895Z","iopub.execute_input":"2024-12-22T08:39:25.481217Z","iopub.status.idle":"2024-12-22T08:39:25.527839Z","shell.execute_reply.started":"2024-12-22T08:39:25.481196Z","shell.execute_reply":"2024-12-22T08:39:25.527109Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"seq2seq_BATCH_SIZE = 64\nseq2seq_EMB_DIM = 128\nseq2seq_HID_DIM = 258\nseq2seq_N_LAYERS = 2\nseq2seq_DROPOUT = 0.5\nseq2seq_LEARNING_RATE = 0.001\nseq2seq_EPOCHS = 10\nseq2seq_FRAC = 0.2\nseq2seq_SEED = 420","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:25.529148Z","iopub.execute_input":"2024-12-22T08:39:25.529427Z","iopub.status.idle":"2024-12-22T08:39:25.542725Z","shell.execute_reply.started":"2024-12-22T08:39:25.529406Z","shell.execute_reply":"2024-12-22T08:39:25.541995Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"transformer_BATCH_SIZE = 32\ntransformer_EMB_DIM = 128\ntransformer_HID_DIM = 258\ntransformer_N_LAYERS = 2\ntransformer_N_HEADS = 8\ntransformer_FF_DIM = 512\ntransformer_DROPOUT = 0.1\ntransformer_LEARNING_RATE = 0.001\ntransformer_EPOCHS = 10\ntransformer_FRAC = 0.5\ntransformer_SEED = 420","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:25.543862Z","iopub.execute_input":"2024-12-22T08:39:25.544145Z","iopub.status.idle":"2024-12-22T08:39:25.555827Z","shell.execute_reply.started":"2024-12-22T08:39:25.544112Z","shell.execute_reply":"2024-12-22T08:39:25.555181Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"kaggle_path= \"/kaggle/input/it-en-translation/processed.parquet\"\nloaded_data = pl.read_parquet(kaggle_path)\nprint(\"Data loaded successfully\")\nloaded_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:25.556597Z","iopub.execute_input":"2024-12-22T08:39:25.556862Z","iopub.status.idle":"2024-12-22T08:39:26.120742Z","shell.execute_reply.started":"2024-12-22T08:39:25.556837Z","shell.execute_reply":"2024-12-22T08:39:26.119860Z"}},"outputs":[{"name":"stdout","text":"Data loaded successfully\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"shape: (5, 2)\n┌─────────────────────────────────┬─────────────────────────────────┐\n│ it                              ┆ en                              │\n│ ---                             ┆ ---                             │\n│ list[str]                       ┆ list[str]                       │\n╞═════════════════════════════════╪═════════════════════════════════╡\n│ [\"grazie\", \"amico\"]             ┆ [\"thank\", \"buddy\"]              │\n│ [\"di il\"]                       ┆ [\"say\"]                         │\n│ [\"trifosfare\", \"sodio\", … \"sod… ┆ [\"sodium\", \"triphosphate\", … \"… │\n│ [\"invero\", \"avidare\", … \"ricch… ┆ [\"surely\", \"ardent\", … \"wealth… │\n│ [\"allegare\"]                    ┆ [\"annex\"]                       │\n└─────────────────────────────────┴─────────────────────────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>it</th><th>en</th></tr><tr><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>[&quot;grazie&quot;, &quot;amico&quot;]</td><td>[&quot;thank&quot;, &quot;buddy&quot;]</td></tr><tr><td>[&quot;di il&quot;]</td><td>[&quot;say&quot;]</td></tr><tr><td>[&quot;trifosfare&quot;, &quot;sodio&quot;, … &quot;sodio&quot;]</td><td>[&quot;sodium&quot;, &quot;triphosphate&quot;, … &quot;tripolyphosphate&quot;]</td></tr><tr><td>[&quot;invero&quot;, &quot;avidare&quot;, … &quot;ricchezzo&quot;]</td><td>[&quot;surely&quot;, &quot;ardent&quot;, … &quot;wealth&quot;]</td></tr><tr><td>[&quot;allegare&quot;]</td><td>[&quot;annex&quot;]</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"transformer_input_data = loaded_data.sample(fraction = transformer_FRAC, seed = transformer_SEED)\n\ntransformer_input_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:26.121592Z","iopub.execute_input":"2024-12-22T08:39:26.121857Z","iopub.status.idle":"2024-12-22T08:39:26.414769Z","shell.execute_reply.started":"2024-12-22T08:39:26.121830Z","shell.execute_reply":"2024-12-22T08:39:26.414044Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"shape: (9, 3)\n┌────────────┬──────────┬──────────┐\n│ statistic  ┆ it       ┆ en       │\n│ ---        ┆ ---      ┆ ---      │\n│ str        ┆ f64      ┆ f64      │\n╞════════════╪══════════╪══════════╡\n│ count      ┆ 480021.0 ┆ 480021.0 │\n│ null_count ┆ 0.0      ┆ 0.0      │\n│ mean       ┆ null     ┆ null     │\n│ std        ┆ null     ┆ null     │\n│ min        ┆ null     ┆ null     │\n│ 25%        ┆ null     ┆ null     │\n│ 50%        ┆ null     ┆ null     │\n│ 75%        ┆ null     ┆ null     │\n│ max        ┆ null     ┆ null     │\n└────────────┴──────────┴──────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>it</th><th>en</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>480021.0</td><td>480021.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>null</td><td>null</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"seq2seq_input_data = loaded_data.sample(fraction = seq2seq_FRAC, seed = seq2seq_SEED)\n\nseq2seq_input_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:26.415564Z","iopub.execute_input":"2024-12-22T08:39:26.415871Z","iopub.status.idle":"2024-12-22T08:39:26.527527Z","shell.execute_reply.started":"2024-12-22T08:39:26.415839Z","shell.execute_reply":"2024-12-22T08:39:26.526753Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"shape: (9, 3)\n┌────────────┬──────────┬──────────┐\n│ statistic  ┆ it       ┆ en       │\n│ ---        ┆ ---      ┆ ---      │\n│ str        ┆ f64      ┆ f64      │\n╞════════════╪══════════╪══════════╡\n│ count      ┆ 192008.0 ┆ 192008.0 │\n│ null_count ┆ 0.0      ┆ 0.0      │\n│ mean       ┆ null     ┆ null     │\n│ std        ┆ null     ┆ null     │\n│ min        ┆ null     ┆ null     │\n│ 25%        ┆ null     ┆ null     │\n│ 50%        ┆ null     ┆ null     │\n│ 75%        ┆ null     ┆ null     │\n│ max        ┆ null     ┆ null     │\n└────────────┴──────────┴──────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>it</th><th>en</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>192008.0</td><td>192008.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>null</td><td>null</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def build_vocab(tokens):\n    vocab = defaultdict(lambda: len(vocab))  # Assign unique indices\n    vocab[\"<pad>\"]  # Reserve 0 for padding\n    vocab[\"<unk>\"]\n    vocab[\"<eos>\"]\n    vocab[\"<bos>\"]\n    for token_list in tokens:\n        for token in token_list:\n            _ = vocab[token]\n    return dict(vocab)\n\ndef preprocess_data(df):\n    it_vocab = build_vocab(df[\"it\"])\n    en_vocab = build_vocab(df[\"en\"])\n    \n    it_indices = [\n        torch.tensor([it_vocab[token] for token in tokens], dtype=torch.long)\n        for tokens in df[\"it\"]\n    ]\n    en_indices = [\n        torch.tensor([en_vocab[token] for token in tokens], dtype=torch.long)\n        for tokens in df[\"en\"]\n    ]\n    return list(zip(en_indices, it_indices)), len(it_vocab), len(en_vocab), en_vocab, it_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:26.530141Z","iopub.execute_input":"2024-12-22T08:39:26.530457Z","iopub.status.idle":"2024-12-22T08:39:26.537777Z","shell.execute_reply.started":"2024-12-22T08:39:26.530432Z","shell.execute_reply":"2024-12-22T08:39:26.537051Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"seq2_seq_data_pairs, seq2seq_IT_VOCAB_SIZE, seq2seq_EN_VOCAB_SIZE, seq2seq_en_v, seq2seq_it_v = preprocess_data(seq2seq_input_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:26.539076Z","iopub.execute_input":"2024-12-22T08:39:26.539361Z","iopub.status.idle":"2024-12-22T08:39:34.136448Z","shell.execute_reply.started":"2024-12-22T08:39:26.539322Z","shell.execute_reply":"2024-12-22T08:39:34.135737Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"transformer_data_pairs, transformer_IT_VOCAB_SIZE, transformer_EN_VOCAB_SIZE, transformer_en_v, transformer_it_v = preprocess_data(transformer_input_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:34.137168Z","iopub.execute_input":"2024-12-22T08:39:34.137424Z","iopub.status.idle":"2024-12-22T08:39:52.788522Z","shell.execute_reply.started":"2024-12-22T08:39:34.137403Z","shell.execute_reply":"2024-12-22T08:39:52.787787Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, trg, hidden, cell):\n        trg = trg.unsqueeze(1)\n        embedded = self.dropout(self.embedding(trg))\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:52.789354Z","iopub.execute_input":"2024-12-22T08:39:52.789569Z","iopub.status.idle":"2024-12-22T08:39:52.796316Z","shell.execute_reply.started":"2024-12-22T08:39:52.789550Z","shell.execute_reply":"2024-12-22T08:39:52.795300Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.fc_out.out_features\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        hidden, cell = self.encoder(src)\n\n        trg_input = trg[:, 0]\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(trg_input, hidden, cell)\n            outputs[:, t, :] = output\n            top1 = output.argmax(1)\n            trg_input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:52.797228Z","iopub.execute_input":"2024-12-22T08:39:52.797621Z","iopub.status.idle":"2024-12-22T08:39:52.812342Z","shell.execute_reply.started":"2024-12-22T08:39:52.797589Z","shell.execute_reply":"2024-12-22T08:39:52.811724Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"seq2seq_encoder = Encoder(seq2seq_EN_VOCAB_SIZE, seq2seq_EMB_DIM, seq2seq_HID_DIM, seq2seq_N_LAYERS, seq2seq_DROPOUT)\nseq2seq_decoder = Decoder(seq2seq_IT_VOCAB_SIZE, seq2seq_EMB_DIM, seq2seq_HID_DIM, seq2seq_N_LAYERS, seq2seq_DROPOUT)\nseq2seq_model = Seq2Seq(seq2seq_encoder, seq2seq_decoder, device).to(device)\n\nseq2seq_model.load_state_dict(torch.load(\"/kaggle/input/dec-enc-test/seq2seq_translation_model.pth\", weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:52.813102Z","iopub.execute_input":"2024-12-22T08:39:52.813419Z","iopub.status.idle":"2024-12-22T08:39:56.073833Z","shell.execute_reply.started":"2024-12-22T08:39:52.813390Z","shell.execute_reply":"2024-12-22T08:39:56.073083Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, trg_vocab_size, emb_dim, n_heads, ff_dim, n_layers, dropout):\n        super().__init__()\n        # Ensure src_vocab_size matches the vocabulary size from preprocessing\n        self.src_embedding = nn.Embedding(src_vocab_size, emb_dim)\n        # Ensure trg_vocab_size matches the vocabulary size from preprocessing\n        self.trg_embedding = nn.Embedding(trg_vocab_size, emb_dim)\n        # Check that the positional encoding length covers all sequence lengths\n        self.positional_encoding = self._get_positional_encoding(emb_dim)\n\n        self.transformer = nn.Transformer(\n            d_model=emb_dim,\n            nhead=n_heads,\n            num_encoder_layers=n_layers,\n            num_decoder_layers=n_layers,\n            dim_feedforward=ff_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.fc_out = nn.Linear(emb_dim, trg_vocab_size)\n\n    def forward(self, src, trg):\n        src_seq_len = src.size(1)  # Sequence length of source\n        src = self.src_embedding(src) + self.positional_encoding[:, :src_seq_len, :].to(src.device)\n        trg_seq_len = trg.size(1)  # Sequence length of target\n        trg = self.trg_embedding(trg) + self.positional_encoding[:, :trg_seq_len, :].to(trg.device)\n\n        src_mask = self._generate_square_subsequent_mask(src_seq_len).to(src.device)\n        trg_mask = self._generate_square_subsequent_mask(trg_seq_len).to(trg.device)\n        memory_mask = None\n\n        # Ensure src, trg, and masks are aligned\n        output = self.transformer(\n            src, trg, src_mask=src_mask, tgt_mask=trg_mask, memory_mask=memory_mask\n        )\n        output = self.fc_out(output)\n\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n\n    def _get_positional_encoding(self, emb_dim, max_len=5000):\n        pe = torch.zeros(1, max_len, emb_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / emb_dim))\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        return pe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:56.074678Z","iopub.execute_input":"2024-12-22T08:39:56.074985Z","iopub.status.idle":"2024-12-22T08:39:56.083155Z","shell.execute_reply.started":"2024-12-22T08:39:56.074953Z","shell.execute_reply":"2024-12-22T08:39:56.082370Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"transformer_model = Transformer(transformer_EN_VOCAB_SIZE, transformer_IT_VOCAB_SIZE, transformer_EMB_DIM, transformer_N_HEADS, transformer_FF_DIM, transformer_N_LAYERS, transformer_DROPOUT).to(device)\n\ntransformer_model.load_state_dict(torch.load(\"/kaggle/input/transformer-translator/transformer_translation_model.pth\", weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:56.083906Z","iopub.execute_input":"2024-12-22T08:39:56.084091Z","iopub.status.idle":"2024-12-22T08:39:57.999593Z","shell.execute_reply.started":"2024-12-22T08:39:56.084073Z","shell.execute_reply":"2024-12-22T08:39:57.998866Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"seq2seq_it_v_inv = {v: k for k, v in seq2seq_it_v.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.000323Z","iopub.execute_input":"2024-12-22T08:39:58.000539Z","iopub.status.idle":"2024-12-22T08:39:58.010819Z","shell.execute_reply.started":"2024-12-22T08:39:58.000521Z","shell.execute_reply":"2024-12-22T08:39:58.009942Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"transformer_it_v_inv = {v: k for k, v in transformer_it_v.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.011590Z","iopub.execute_input":"2024-12-22T08:39:58.011817Z","iopub.status.idle":"2024-12-22T08:39:58.036910Z","shell.execute_reply.started":"2024-12-22T08:39:58.011799Z","shell.execute_reply":"2024-12-22T08:39:58.036230Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def tokenize(sentence, vocab):\n    tokens = sentence.lower().split()  # Basic tokenization; adjust if needed\n    return torch.tensor([vocab.get(token, vocab[\"<unk>\"]) for token in tokens], dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.037681Z","iopub.execute_input":"2024-12-22T08:39:58.037966Z","iopub.status.idle":"2024-12-22T08:39:58.050477Z","shell.execute_reply.started":"2024-12-22T08:39:58.037938Z","shell.execute_reply":"2024-12-22T08:39:58.049826Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def seq2seq_translate(sentence, model, en_vocab, it_vocab, it_vocab_inv, device=\"cuda\"):\n    model.eval()\n    model.to(device)\n\n    # Tokenize and convert the input sentence into a tensor\n    input_tensor = tokenize(sentence, en_vocab).unsqueeze(0).to(device)\n\n    # Pass the source sentence through the encoder\n    hidden, cell = model.encoder(input_tensor)\n\n    # Initialize the decoder input with the <bos> token\n    decoder_input = torch.tensor([it_vocab[\"<bos>\"]], dtype=torch.long, device=device)\n\n    # Store decoded tokens\n    decoded_tokens = []\n\n    for _ in range(50):  # Limit translation to 50 tokens\n        # Pass through the decoder\n        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n\n        # Get the predicted token\n        next_token_idx = output.argmax(1).item()\n\n        # Stop if <eos> is generated\n        if next_token_idx == it_vocab[\"<eos>\"]:\n            break\n\n        # Append the token to the output sequence\n        decoded_tokens.append(it_vocab_inv[next_token_idx])\n\n        # Update the decoder input to the predicted token\n        decoder_input = torch.tensor([next_token_idx], dtype=torch.long, device=device)\n\n    # Join decoded tokens to form the translated sentence\n    return \" \".join(decoded_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.051268Z","iopub.execute_input":"2024-12-22T08:39:58.051576Z","iopub.status.idle":"2024-12-22T08:39:58.067110Z","shell.execute_reply.started":"2024-12-22T08:39:58.051549Z","shell.execute_reply":"2024-12-22T08:39:58.066435Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def transformer_translate(sentence, model, en_vocab, it_vocab, it_vocab_inv, device=\"cuda\"):\n    model.eval()\n    model.to(device)\n\n    # Tokenize and convert the input sentence into a tensor\n    input_tensor = tokenize(sentence, en_vocab).unsqueeze(0).to(device)\n    src_seq_len = input_tensor.size(1)\n    src_mask = model.generate_square_subsequent_mask(src_seq_len).to(device)\n\n    # Pass the source sentence through the transformer\n    src = model.src_embedding(input_tensor) + model.positional_encoding[:, :src_seq_len, :].to(device)\n    memory = model.transformer.encoder(src, mask=src_mask)\n\n    # Initialize the decoder input with the <bos> token\n    decoder_input = torch.tensor([it_vocab[\"<bos>\"]], dtype=torch.long, device=device).unsqueeze(0)\n    decoded_tokens = []\n\n    for _ in range(50):  # Limit translation to 50 tokens\n        tgt_seq_len = decoder_input.size(1)\n        tgt_mask = model.generate_square_subsequent_mask(tgt_seq_len).to(device)\n\n        trg = model.trg_embedding(decoder_input) + model.positional_encoding[:, :tgt_seq_len, :].to(device)\n        output = model.transformer.decoder(trg, memory, tgt_mask=tgt_mask)\n        output = model.fc_out(output)\n\n        # Get the next token\n        next_token_idx = output[:, -1, :].argmax(1).item()\n\n        # Stop if <eos> is predicted\n        if next_token_idx == it_vocab[\"<eos>\"]:\n            break\n\n        decoded_tokens.append(it_vocab_inv[next_token_idx])\n\n        # Append the predicted token to the decoder input\n        decoder_input = torch.cat(\n            [decoder_input, torch.tensor([[next_token_idx]], dtype=torch.long, device=device)], dim=1\n        )\n\n    # Join decoded tokens to form the translated sentence\n    return \" \".join(decoded_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.067957Z","iopub.execute_input":"2024-12-22T08:39:58.068258Z","iopub.status.idle":"2024-12-22T08:39:58.080056Z","shell.execute_reply.started":"2024-12-22T08:39:58.068231Z","shell.execute_reply":"2024-12-22T08:39:58.079239Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def translate(sentence, model_type=\"seq2seq\", device=\"cuda\"):\n    if model_type == \"seq2seq\":\n        return seq2seq_translate(sentence, seq2seq_model, seq2seq_en_v, seq2seq_it_v, seq2seq_it_v_inv, device)\n    elif model_type == \"transformer\":\n        return transformer_translate(sentence, transformer_model, transformer_en_v, transformer_it_v, transformer_it_v_inv, device)\n    else:\n        raise ValueError(\"Invalid model_type. Choose 'seq2seq' or 'transformer'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.080743Z","iopub.execute_input":"2024-12-22T08:39:58.080959Z","iopub.status.idle":"2024-12-22T08:39:58.094491Z","shell.execute_reply.started":"2024-12-22T08:39:58.080941Z","shell.execute_reply":"2024-12-22T08:39:58.093713Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"sentence = \"How are you doing?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.095165Z","iopub.execute_input":"2024-12-22T08:39:58.095401Z","iopub.status.idle":"2024-12-22T08:39:58.108742Z","shell.execute_reply.started":"2024-12-22T08:39:58.095382Z","shell.execute_reply":"2024-12-22T08:39:58.108012Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"seq2seq_translation = translate(sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.111611Z","iopub.execute_input":"2024-12-22T08:39:58.111797Z","iopub.status.idle":"2024-12-22T08:39:58.342219Z","shell.execute_reply.started":"2024-12-22T08:39:58.111781Z","shell.execute_reply":"2024-12-22T08:39:58.341551Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(\"Sentence Translation (Using Seq2Seq Model): \", seq2seq_translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.343097Z","iopub.execute_input":"2024-12-22T08:39:58.343324Z","iopub.status.idle":"2024-12-22T08:39:58.347725Z","shell.execute_reply.started":"2024-12-22T08:39:58.343306Z","shell.execute_reply":"2024-12-22T08:39:58.346984Z"}},"outputs":[{"name":"stdout","text":"Sentence Translation (Using Seq2Seq Model):  \n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"transformer_translation = translate(sentence, model_type = \"transformer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.348547Z","iopub.execute_input":"2024-12-22T08:39:58.348843Z","iopub.status.idle":"2024-12-22T08:39:58.553058Z","shell.execute_reply.started":"2024-12-22T08:39:58.348814Z","shell.execute_reply":"2024-12-22T08:39:58.552167Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(\"Sentence Translation (Using Transformer Model): \", transformer_translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:39:58.554039Z","iopub.execute_input":"2024-12-22T08:39:58.554323Z","iopub.status.idle":"2024-12-22T08:39:58.558759Z","shell.execute_reply.started":"2024-12-22T08:39:58.554274Z","shell.execute_reply":"2024-12-22T08:39:58.557992Z"}},"outputs":[{"name":"stdout","text":"Sentence Translation (Using Transformer Model):  ares americano\n","output_type":"stream"}],"execution_count":26}]}