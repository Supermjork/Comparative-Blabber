{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Neural Machine Translation\n",
    "\n",
    "This notebook implements a sequence-to-sequence model for machine translation. The implementation is organized into modular sections for clarity and maintainability.\n",
    "\n",
    "## Table of Contents\n",
    "1. Imports and Setup\n",
    "2. Data Handling\n",
    "3. Model Architecture\n",
    "4. Training Utilities\n",
    "5. Translation Utilities\n",
    "6. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Handling\n",
    "\n",
    "### 2.1 Vocabulary Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(token_list: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], int]:\n",
    "    \"\"\"Build vocabulary from list of tokens.\"\"\"\n",
    "    # Count token frequencies\n",
    "    counter = Counter(token_list)\n",
    "    \n",
    "    # Filter by minimum frequency\n",
    "    filtered_tokens = [token for token, count in counter.items() if count >= min_freq]\n",
    "    \n",
    "    # Create vocabulary with special tokens\n",
    "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(special_tokens + filtered_tokens)}\n",
    "    \n",
    "    return token_to_idx, len(token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, src_vocab, tgt_vocab, src_col=\"en\", tgt_col=\"it\"):\n",
    "        self.src_sentences = dataframe[src_col].to_list()\n",
    "        self.tgt_sentences = dataframe[tgt_col].to_list()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert tokens to indices using vocabulary\n",
    "        src_indices = [self.src_vocab.get(token, 0) for token in self.src_sentences[idx]]\n",
    "        tgt_indices = [self.tgt_vocab.get(token, 0) for token in self.tgt_sentences[idx]]\n",
    "        \n",
    "        return {\n",
    "            \"src\": torch.tensor(src_indices, dtype=torch.long),\n",
    "            \"tgt\": torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for padding sequences in a batch.\"\"\"\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "\n",
    "    # Pad sequences in the batch to the same length\n",
    "    src_padded = rnn_utils.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = rnn_utils.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\"src\": src_padded, \"tgt\": tgt_padded}\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size, shuffle=True):\n",
    "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, emb_dim: int, \n",
    "                 hid_dim: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.decoder = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Encode source\n",
    "        src_emb = self.dropout(self.encoder(src))\n",
    "        _, (hidden, cell) = self.rnn(src_emb)\n",
    "\n",
    "        # Decode target\n",
    "        tgt_emb = self.dropout(self.decoder(tgt))\n",
    "        outputs, _ = self.rnn(tgt_emb, (hidden, cell))\n",
    "\n",
    "        predictions = self.fc_out(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: torch.device\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt[:, :-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def validate_model(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"Validate the model and return validation loss.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_losses: List[float]):\n",
    "    \"\"\"Plot training and validation loss history.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training History\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Translation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    model: nn.Module,\n",
    "    sentence: list,\n",
    "    src_vocab: dict,\n",
    "    tgt_vocab: dict,\n",
    "    device: torch.device,\n",
    "    max_len: int = 50\n",
    ") -> list:\n",
    "    \"\"\"Translate a sentence using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    src_indices = [src_vocab.get(token, src_vocab['<unk>']) for token in sentence]\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.encoder(src_tensor)\n",
    "        _, (hidden, cell) = model.rnn(src_emb)\n",
    "\n",
    "    # Initialize with start token\n",
    "    outputs = [tgt_vocab['<sos>']]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(outputs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(tgt_tensor)\n",
    "            output, (hidden, cell) = model.rnn(output, (hidden, cell))\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "        outputs.append(pred_token)\n",
    "        if pred_token == tgt_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    idx_to_token = {idx: token for token, idx in tgt_vocab.items()}\n",
    "    translated_tokens = [idx_to_token.get(idx, '<unk>') for idx in outputs[1:-1]]  # Remove <sos> and <eos>\n",
    "    \n",
    "    return translated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>it</th><th>en</th></tr><tr><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>[&quot;grazie&quot;, &quot;amico&quot;]</td><td>[&quot;thank&quot;, &quot;buddy&quot;]</td></tr><tr><td>[&quot;di il&quot;]</td><td>[&quot;say&quot;]</td></tr><tr><td>[&quot;trifosfare&quot;, &quot;sodio&quot;, … &quot;sodio&quot;]</td><td>[&quot;sodium&quot;, &quot;triphosphate&quot;, … &quot;tripolyphosphate&quot;]</td></tr><tr><td>[&quot;invero&quot;, &quot;avidare&quot;, … &quot;ricchezzo&quot;]</td><td>[&quot;surely&quot;, &quot;ardent&quot;, … &quot;wealth&quot;]</td></tr><tr><td>[&quot;allegare&quot;]</td><td>[&quot;annex&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ it                              ┆ en                              │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ list[str]                       ┆ list[str]                       │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ [\"grazie\", \"amico\"]             ┆ [\"thank\", \"buddy\"]              │\n",
       "│ [\"di il\"]                       ┆ [\"say\"]                         │\n",
       "│ [\"trifosfare\", \"sodio\", … \"sod… ┆ [\"sodium\", \"triphosphate\", … \"… │\n",
       "│ [\"invero\", \"avidare\", … \"ricch… ┆ [\"surely\", \"ardent\", … \"wealth… │\n",
       "│ [\"allegare\"]                    ┆ [\"annex\"]                       │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "\n",
    "# Load data\n",
    "input_ready = pl.read_ndjson(\"../data/output/processed.json\")\n",
    "print(\"Data loaded successfully\")\n",
    "input_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 130049\n",
      "Italian Vocabulary Size: 198442\n"
     ]
    }
   ],
   "source": [
    "# Build vocabularies\n",
    "english_tokens = [token for sentence in input_ready[\"en\"].to_list() for token in sentence]\n",
    "italian_tokens = [token for sentence in input_ready[\"it\"].to_list() for token in sentence]\n",
    "\n",
    "english_vocab, input_dim = build_vocab(english_tokens)\n",
    "italian_vocab, output_dim = build_vocab(italian_tokens)\n",
    "\n",
    "print(f\"English Vocabulary Size: {input_dim}\")\n",
    "print(f\"Italian Vocabulary Size: {output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and dataloaders created successfully\n"
     ]
    }
   ],
   "source": [
    "# Split data and create datasets\n",
    "train_data, val_data = train_test_split(input_ready, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data, english_vocab, italian_vocab)\n",
    "val_dataset = TranslationDataset(val_data, english_vocab, italian_vocab)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, BATCH_SIZE)\n",
    "print(\"Datasets and dataloaders created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "model = Seq2Seq(input_dim, output_dim, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, device)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/translator.pt\")\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "test_sentence = [\"hello\", \"world\"]\n",
    "translated = translate_sentence(model, test_sentence, english_vocab, italian_vocab, device)\n",
    "print(f\"Input: {' '.join(test_sentence)}\")\n",
    "print(f\"Translation: {' '.join(translated)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
